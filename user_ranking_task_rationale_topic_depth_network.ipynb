{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#method_raw_text = pd.read_excel('/Users/angieryu2202/Desktop/IF_App_Study/Yes_Responses.xlsx')\n",
    "method_raw_text = pd.read_excel('/Users/angieryu2202/Desktop/IF_App_Study/No_Responses.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Related Sentences in Reviews - Co-Word Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all newlines from dataframe\n",
    "method_raw_text = method_raw_text.replace('\\n','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_raw_text = method_raw_text.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for line in method_raw_text['Responses']:\n",
    "    substring = re.sub(r'[^\\w\\s]','',str(line))\n",
    "    #substring = ''.join([i for i in str(substring) if not i.isdigit()])\n",
    "    substring = str(substring).lower().replace(\"$\", \"\").replace(\"/\",\"\")\n",
    "    method_raw_text['Responses'] = method_raw_text['Responses'].replace(line, substring)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor (text_column, column_name):\n",
    "    import re, string\n",
    "    from nltk import word_tokenize, pos_tag\n",
    "    from nltk.corpus import stopwords\n",
    "    from stop_words import get_stop_words\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "    nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "    stop_words.extend(nltk_words)\n",
    "    #remove_words = ['payment', 'price', 'debit', 'card', 'account', 'tbh', 'subscription', 'app', 'application', 'youtube', 'username', 'password', 'yr', 'ur', 'id', 'isnt', 'wouldnt', 'doesnt', 'accountyou', 'im', 'thats', 'logins', 'wont', 'didnt', 'ive', 'ill', 'youre']\n",
    "    #stop_words.extend(remove_words)\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    globals()[column_name+'_filtered_sent']=[]\n",
    "    for line in text_column:\n",
    "        word_tokens = word_tokenize(re.sub('[%s]' % re.escape(string.punctuation), '', str(line)))\n",
    "        nouns = [token for token, pos in pos_tag(word_tokens) if pos.startswith('NN')]\n",
    "        adjectives = [token for token, pos in pos_tag(word_tokens) if pos.startswith('JJ')]\n",
    "        lemmatized = []\n",
    "        for noun in nouns:\n",
    "            lemmatized.append(lemmatizer.lemmatize(noun))\n",
    "        for adjective in adjectives:\n",
    "            lemmatized.append(lemmatizer.lemmatize(adjective))\n",
    "        filtered_sentence = [w for w in lemmatized if not w in stop_words]\n",
    "        globals()[column_name+'_filtered_sent'].append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor(method_raw_text['Responses'], 'reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['info', 'blurb', 'article', 'short', 'full'],\n",
       " ['order', 'convenience', 'design'],\n",
       " ['number',\n",
       "  'diet',\n",
       "  'article',\n",
       "  'number',\n",
       "  'faq',\n",
       "  'page',\n",
       "  'intermittent',\n",
       "  'fasting',\n",
       "  'health',\n",
       "  'concern',\n",
       "  'number',\n",
       "  'information',\n",
       "  'ill',\n",
       "  'number',\n",
       "  'others',\n",
       "  'information',\n",
       "  'useful',\n",
       "  'useful',\n",
       "  'new',\n",
       "  'many',\n",
       "  'general',\n",
       "  'le',\n",
       "  'general'],\n",
       " ['article',\n",
       "  'subject',\n",
       "  'lot',\n",
       "  'time',\n",
       "  'assumption',\n",
       "  'information',\n",
       "  'question',\n",
       "  'answer',\n",
       "  'access',\n",
       "  'research',\n",
       "  'related',\n",
       "  'irrelevant',\n",
       "  'dont',\n",
       "  'good',\n",
       "  'authoritative',\n",
       "  'short',\n",
       "  'definitive',\n",
       "  'quick',\n",
       "  'easy',\n",
       "  'supplemental'],\n",
       " ['graphic',\n",
       "  'image',\n",
       "  'text',\n",
       "  'format',\n",
       "  'time',\n",
       "  'user',\n",
       "  'idea',\n",
       "  'kind',\n",
       "  'information',\n",
       "  'service',\n",
       "  'app',\n",
       "  'offer',\n",
       "  'uiux',\n",
       "  'intuitive',\n",
       "  'full',\n",
       "  'text',\n",
       "  'much'],\n",
       " ['option',\n",
       "  'home',\n",
       "  'page',\n",
       "  'article',\n",
       "  'contains',\n",
       "  'image',\n",
       "  'amount',\n",
       "  'text',\n",
       "  'slide',\n",
       "  'legibility',\n",
       "  'information',\n",
       "  'content',\n",
       "  'method',\n",
       "  'option',\n",
       "  'option',\n",
       "  'interface',\n",
       "  'term',\n",
       "  'information',\n",
       "  'option',\n",
       "  'side',\n",
       "  'bar',\n",
       "  'reduction',\n",
       "  'click',\n",
       "  'option',\n",
       "  'rest',\n",
       "  'preference',\n",
       "  'option',\n",
       "  'rest',\n",
       "  'clutter',\n",
       "  'image',\n",
       "  'information',\n",
       "  'way',\n",
       "  'first',\n",
       "  'actual',\n",
       "  'right',\n",
       "  'first',\n",
       "  'second',\n",
       "  'better',\n",
       "  'user',\n",
       "  'additional',\n",
       "  'extra',\n",
       "  'second',\n",
       "  'last',\n",
       "  'le'],\n",
       " ['impression', 'rank', 'first', 'likely'],\n",
       " ['experience'],\n",
       " ['one', 'others', 'first', 'familiar'],\n",
       " ['gut', 'isgreat'],\n",
       " ['thats', 'way'],\n",
       " ['information', 'format', 'easy'],\n",
       " ['easiest', 'convenient'],\n",
       " ['simple'],\n",
       " ['ranking', 'video', 'interested'],\n",
       " ['convenient'],\n",
       " ['friendly'],\n",
       " ['way', 'easier'],\n",
       " ['feeling', 'issue', 'fit', 'overall'],\n",
       " ['way'],\n",
       " ['informative'],\n",
       " ['simple', 'opinion', 'way', 'anything', 'best', 'easiest'],\n",
       " ['way']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_filtered_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "class Cooccurrence(CountVectorizer):\n",
    "    \"\"\"Co-ocurrence matrix\n",
    "    Convert collection of raw documents to word-word co-ocurrence matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : string, 'utf-8' by default.\n",
    "        If bytes or files are given to analyze, this encoding is used to\n",
    "        decode.\n",
    "\n",
    "    ngram_range : tuple (min_n, max_n)\n",
    "        The lower and upper boundary of the range of n-values for different\n",
    "        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "        will be used.\n",
    "\n",
    "    max_df: float in range [0, 1] or int, default=1.0\n",
    "\n",
    "    min_df: float in range [0, 1] or int, default=1\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "\n",
    "    >> import Cooccurrence\n",
    "    >> docs = ['this book is good',\n",
    "               'this cat is good',\n",
    "               'cat is good shit']\n",
    "    >> model = Cooccurrence()\n",
    "    >> Xc = model.fit_transform(docs)\n",
    "\n",
    "    Check vocabulary by printing\n",
    "    >> model.vocabulary_\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='utf-8', ngram_range=(1, 1),\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 stop_words=None, normalize=True, vocabulary=None):\n",
    "\n",
    "        super(Cooccurrence, self).__init__(\n",
    "            ngram_range=ngram_range,\n",
    "            max_df=max_df,\n",
    "            min_df=min_df,\n",
    "            max_features=max_features,\n",
    "            stop_words=stop_words,\n",
    "            vocabulary=vocabulary\n",
    "        )\n",
    "\n",
    "        self.X = None\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Fit cooccurrence matrix\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            an iterable which yields either str, unicode or file objects\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Xc : Cooccurrence matrix\n",
    "\n",
    "        \"\"\"\n",
    "        X = super(Cooccurrence, self).fit_transform(raw_documents)\n",
    "        self.X = X\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        Xc = (X.T * X)\n",
    "        if self.normalize:\n",
    "            g = sp.diags(1./Xc.diagonal())\n",
    "            Xc = g * Xc\n",
    "        else:\n",
    "            Xc.setdiag(0)\n",
    "\n",
    "        return Xc\n",
    "\n",
    "    def vocab(self):\n",
    "        tuples = super(Cooccurrence, self).get_feature_names()\n",
    "        vocabulary=[]\n",
    "        for e_tuple in tuples:\n",
    "            tokens = e_tuple.split()\n",
    "            for t in tokens:\n",
    "                if t not in vocabulary:\n",
    "                    vocabulary.append(t)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    def word_histgram(self):\n",
    "        word_list = super(Cooccurrence, self).get_feature_names()\n",
    "        count_list = self.X.toarray().sum(axis=0)\n",
    "        return dict(zip(word_list,count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "class BaseCooccurrence:\n",
    "    INPUT=[list,str]\n",
    "    OUTPUT=[list,tuple]\n",
    "\n",
    "class CooccurrenceWorker(BaseCooccurrence):\n",
    "    def __init__(self):\n",
    "        name = 'cooccurrence'\n",
    "        self.inst = Cooccurrence(ngram_range=(2, 2), stop_words='english')\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "\n",
    "        # bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), vocabulary={'awesome unicorns': 0, 'batman forever': 1})\n",
    "        co_occurrences = self.inst.fit_transform(args[0])\n",
    "        # print('Printing sparse matrix:', co_occurrences)\n",
    "        # print(co_occurrences.todense())\n",
    "        sum_occ = np.sum(co_occurrences.todense(), axis=0)\n",
    "        # print('Sum of word-word occurrences:', sum_occ)\n",
    "\n",
    "        # Converting itertor to set\n",
    "        result = zip(self.inst.get_feature_names(), np.array(sum_occ)[0].tolist())\n",
    "        result_set = list(result)\n",
    "        return result_set, self.inst.vocab()\n",
    "\n",
    "class CooccurrenceManager:\n",
    "    def computeCooccurence(self, list):\n",
    "        com = defaultdict(lambda: defaultdict(int))\n",
    "        count_all = Counter()\n",
    "        count_all1 = Counter()\n",
    "\n",
    "        uniqueList = []\n",
    "        for _array in list:\n",
    "            for line in _array:\n",
    "                for word in line:\n",
    "                    if word not in uniqueList:\n",
    "                        uniqueList.append(word)\n",
    "\n",
    "                terms_bigram = bigrams(line)\n",
    "                # Update the counter\n",
    "                count_all.update(line)\n",
    "                count_all1.update(terms_bigram)\n",
    "\n",
    "                # Build co-occurrence matrix\n",
    "                for i in range(len(line) - 1):\n",
    "                    for j in range(i + 1, len(line)):\n",
    "                        w1, w2 = sorted([line[i], line[j]])\n",
    "                        if w1 != w2:\n",
    "                            com[w1][w2] += 1\n",
    "\n",
    "        com_max = []\n",
    "        # For each term, look for the most common co-occurrent terms\n",
    "        for t1 in com:\n",
    "            t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "            for t2, t2_count in t1_max_terms:\n",
    "                com_max.append(((t1, t2), t2_count))\n",
    "        # Get the most frequent co-occurrences\n",
    "        terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        return terms_max, uniqueList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "co = CooccurrenceWorker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for sublist in reviews_filtered_sent:\n",
    "    document = \",\".join(sublist)\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import itertools\n",
    "#merged = list(itertools.chain(*mecab_nouns))\n",
    "co_result, vocab = co.__call__(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.font_manager as fm\n",
    "import platform\n",
    "from matplotlib.ft2font import FT2Font\n",
    "import matplotlib as mpl\n",
    "\n",
    "class GraphMLCreator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.G = nx.Graph()\n",
    "\n",
    "        # Hack: offset the most central node to avoid too much overlap\n",
    "        self.rad0 = 0.3\n",
    "\n",
    "    def createGraphML(self, co_occurrence, word_hist, vocabulary, file):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        for obj in vocabulary:\n",
    "            G.add_node(obj)\n",
    "        # convert list to a single dictionary\n",
    "\n",
    "        for pair in co_occurrence:\n",
    "            node1 = ''\n",
    "            node2 = ''\n",
    "            for inner_pair in pair:\n",
    "\n",
    "                if type(inner_pair) is tuple:\n",
    "                    node1 = inner_pair[0]\n",
    "                    node2 = inner_pair[1]\n",
    "                elif type(inner_pair) is str:\n",
    "                    inner_pair=inner_pair.split()\n",
    "                    node1 = inner_pair[0]\n",
    "                    node2 = inner_pair[1]\n",
    "                elif type(inner_pair) is int:\n",
    "                    #print (\"X \" + node1 + \" == \" + node2 + \" == \" + str(inner_pair) + \" : \" + str(tuple[node1]))\n",
    "                    G.add_edge(node1, node2, weight=float(inner_pair))\n",
    "                elif type(inner_pair) is float:\n",
    "                    #print (\"X \" + node1 + \" == \" + node2 + \" == \" + str(inner_pair) + \" : \")\n",
    "                    G.add_edge(node1, node2, weight=float(inner_pair))\n",
    "        for word in word_hist:\n",
    "            G.add_node(word, count=word_hist[word])\n",
    "        self.G = G\n",
    "        print(self.G.number_of_nodes())\n",
    "        nx.write_graphml(G, file)\n",
    "\n",
    "    def createGraphMLWithThreshold(self, co_occurrence, word_hist, vocab, file, threshold=10.0):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        filtered_word_list=[]\n",
    "        for pair in co_occurrence:\n",
    "            node1 = ''\n",
    "            node2 = ''\n",
    "            for inner_pair in pair:\n",
    "                if type(inner_pair) is tuple:\n",
    "                    node1 = inner_pair[0]\n",
    "                    node2 = inner_pair[1]\n",
    "                elif type(inner_pair) is str:\n",
    "                    inner_pair=inner_pair.split()\n",
    "                    node1 = inner_pair[0]\n",
    "                    node2 = inner_pair[1]\n",
    "                elif type(inner_pair) is int:\n",
    "                    if float(inner_pair) >= threshold:\n",
    "                        #print (\"X \" + node1 + \" == \" + node2 + \" == \" + str(inner_pair) + \" : \" + str(tuple[node1]))\n",
    "                        G.add_edge(node1, node2, weight=float(inner_pair))\n",
    "                        if node1 not in filtered_word_list:\n",
    "                            filtered_word_list.append(node1)\n",
    "                        if node2 not in filtered_word_list:\n",
    "                            filtered_word_list.append(node2)\n",
    "                elif type(inner_pair) is float:\n",
    "                    if float(inner_pair) >= threshold:\n",
    "                        #print (\"X \" + node1 + \" == \" + node2 + \" == \" + str(inner_pair) + \" : \")\n",
    "                        G.add_edge(node1, node2, weight=float(inner_pair))\n",
    "                        if node1 not in filtered_word_list:\n",
    "                            filtered_word_list.append(node1)\n",
    "                        if node2 not in filtered_word_list:\n",
    "                            filtered_word_list.append(node2)\n",
    "        for word in word_hist:\n",
    "            if word in filtered_word_list:\n",
    "                G.add_node(word, count=word_hist[word])\n",
    "\n",
    "        self.G = G\n",
    "        print(self.G.number_of_nodes())\n",
    "        nx.write_graphml(G, file)\n",
    "\n",
    "    def centrality_layout(self):\n",
    "        centrality = nx.eigenvector_centrality_numpy(self.G)\n",
    "        \"\"\"Compute a layout based on centrality.\n",
    "        \"\"\"\n",
    "        # Create a list of centralities, sorted by centrality value\n",
    "        cent = sorted(centrality.items(), key=lambda x:float(x[1]), reverse=True)\n",
    "        nodes = [c[0] for c in cent]\n",
    "        cent  = np.array([float(c[1]) for c in cent])\n",
    "        rad = (cent - cent[0])/(cent[-1]-cent[0])\n",
    "        rad = self.rescale_arr(rad, self.rad0, 1)\n",
    "        angles = np.linspace(0, 2*np.pi, len(centrality))\n",
    "        layout = {}\n",
    "        for n, node in enumerate(nodes):\n",
    "            r = rad[n]\n",
    "            th = angles[n]\n",
    "            layout[node] = r*np.cos(th), r*np.sin(th)\n",
    "        return layout\n",
    "\n",
    "    def plot_graph(self, title=None, file='graph.png'):\n",
    "        from matplotlib.font_manager import _rebuild\n",
    "        _rebuild()\n",
    "\n",
    "        font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'\n",
    "\n",
    "        font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "        plt.rc('font', family=font_name)\n",
    "        plt.rc('axes', unicode_minus=False)\n",
    "        # 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "        mpl.rcParams['axes.unicode_minus'] = False\n",
    "        #print('버전: ', mpl.__version__)\n",
    "        #print('설치 위치: ', mpl.__file__)\n",
    "        #print('설정 위치: ', mpl.get_configdir())\n",
    "        #print('캐시 위치: ', mpl.get_cachedir())\n",
    "\n",
    "        # size, family\n",
    "        print('# 설정 되어있는 폰트 사이즈')\n",
    "        print(plt.rcParams['font.size'])\n",
    "        print('# 설정 되어있는 폰트 글꼴')\n",
    "        print(plt.rcParams['font.family'])\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        pos = self.centrality_layout()\n",
    "\n",
    "        \"\"\"Conveniently summarize graph visually\"\"\"\n",
    "        # config parameters\n",
    "        edge_min_width= 3\n",
    "        edge_max_width= 12\n",
    "        label_font = 18\n",
    "        node_font = 22\n",
    "        node_alpha = 0.4\n",
    "        edge_alpha = 0.55\n",
    "        edge_cmap = plt.cm.Spectral\n",
    "\n",
    "        # Create figure\n",
    "        if fig is None:\n",
    "            fig, ax = plt.subplots()\n",
    "        else:\n",
    "            ax = fig.add_subplot(111)\n",
    "        fig.subplots_adjust(0,0,1)\n",
    "\n",
    "        font = FT2Font(font_path)\n",
    "\n",
    "        # Plot nodes with size according to count\n",
    "        sizes = []\n",
    "        degrees = []\n",
    "        for n, d in self.G.nodes(data=True):\n",
    "            sizes.append(d['count'])\n",
    "            degrees.append(self.G.degree(n))\n",
    "\n",
    "        sizes = self.rescale_arr(np.array(sizes, dtype=float), 100, 1000)\n",
    "\n",
    "        # Compute layout and label edges according to weight\n",
    "        pos = nx.spectral_layout(self.G) if pos is None else pos\n",
    "        labels = {}\n",
    "        width = []\n",
    "        for n1, n2, d in self.G.edges(data=True):\n",
    "            w = d['weight']\n",
    "            labels[n1, n2] = w\n",
    "            width.append(w)\n",
    "\n",
    "        width = self.rescale_arr(np.array(width, dtype=float), edge_min_width,\n",
    "                            edge_max_width)\n",
    "\n",
    "        # Draw\n",
    "        nx.draw_networkx_nodes(self.G, pos, node_size=sizes, node_color=degrees,\n",
    "                               alpha=node_alpha)\n",
    "        nx.draw_networkx_edges(self.G, pos, width=width, edge_color=width,\n",
    "                               edge_cmap=edge_cmap, alpha=edge_alpha)\n",
    "        #nx.draw_networkx_edge_labels(self.G, pos, edge_labels=labels,\n",
    "                                     #font_size=label_font)\n",
    "        nx.draw_networkx_labels(self.G, pos, font_size=node_font, font_family=font_name, font_weight='bold')\n",
    "\n",
    "        if title is not None:\n",
    "            ax.set_title(title, fontsize=label_font)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Mark centrality axes\n",
    "        kw = dict(color='k', linestyle='-')\n",
    "        cross = [ax.axhline(0, **kw), ax.axvline(self.rad0, **kw)]\n",
    "        [l.set_zorder(0) for l in cross]\n",
    "        \n",
    "        plt.savefig(file)\n",
    "        plt.show()\n",
    "\n",
    "    def rescale_arr(self, arr, amin, amax):\n",
    "        \"\"\"Rescale an array to a new range.\n",
    "        Return a new array whose range of values is (amin, amax).\n",
    "        Parameters\n",
    "        ----------\n",
    "        arr : array-like\n",
    "        amin : float\n",
    "          new minimum value\n",
    "        amax : float\n",
    "          new maximum value\n",
    "        Examples\n",
    "        --------\n",
    "        >>> a = np.arange(5)\n",
    "        >>> rescale_arr(a,3,6)\n",
    "        array([ 3.  ,  3.75,  4.5 ,  5.25,  6.  ])\n",
    "        \"\"\"\n",
    "\n",
    "        # old bounds\n",
    "        m = arr.min()\n",
    "        M = arr.max()\n",
    "        # scale/offset\n",
    "        s = float(amax - amin) / (M - m)\n",
    "        d = amin - s * m\n",
    "\n",
    "        # Apply clip before returning to cut off possible overflows outside the\n",
    "        # intended range due to roundoff error, so that we can absolutely guarantee\n",
    "        # that on output, there are no values > amax or < amin.\n",
    "        return np.clip(s * arr + d, amin, amax)\n",
    "\n",
    "    def summarize_centrality(self, limit=10):\n",
    "        centrality = nx.eigenvector_centrality_numpy(self.G)\n",
    "        c = centrality.items()\n",
    "        c = sorted(c, key=lambda x: x[1], reverse=True)\n",
    "        print('\\nGraph centrality')\n",
    "        count=0\n",
    "        for node, cent in c:\n",
    "            if count>limit:\n",
    "                break\n",
    "            print (\"%15s: %.3g\" % (node, float(cent)))\n",
    "            count+=1\n",
    "\n",
    "    def sort_freqs(self, freqs):\n",
    "        \"\"\"Sort a word frequency histogram represented as a dictionary.\n",
    "        Parameters\n",
    "        ----------\n",
    "        freqs : dict\n",
    "          A dict with string keys and integer values.\n",
    "        Return\n",
    "        ------\n",
    "        items : list\n",
    "          A list of (count, word) pairs.\n",
    "        \"\"\"\n",
    "        items = freqs.items()\n",
    "        items.sort(key=lambda wc: wc[1])\n",
    "        return items\n",
    "\n",
    "    def plot_word_histogram(self, freqs, show=10, title=None):\n",
    "        \"\"\"Plot a histogram of word frequencies, limited to the top `show` ones.\n",
    "        \"\"\"\n",
    "        sorted_f = self.sort_freqs(freqs) if isinstance(freqs, dict) else freqs\n",
    "\n",
    "        # Don't show the tail\n",
    "        if isinstance(show, int):\n",
    "            # interpret as number of words to show in histogram\n",
    "            show_f = sorted_f[-show:]\n",
    "        else:\n",
    "            # interpret as a fraction\n",
    "            start = -int(round(show * len(freqs)))\n",
    "            show_f = sorted_f[start:]\n",
    "\n",
    "        # Now, extract words and counts, plot\n",
    "        n_words = len(show_f)\n",
    "        ind = np.arange(n_words)\n",
    "        words = [i[0] for i in show_f]\n",
    "        counts = [i[1] for i in show_f]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        if n_words <= 20:\n",
    "            # Only show bars and x labels for small histograms, they don't make\n",
    "            # sense otherwise\n",
    "            ax.bar(ind, counts)\n",
    "            ax.set_xticks(ind)\n",
    "            ax.set_xticklabels(words, rotation=45)\n",
    "            fig.subplots_adjust(bottom=0.25)\n",
    "        else:\n",
    "            # For larger ones, do a step plot\n",
    "            ax.step(ind, counts)\n",
    "\n",
    "        # If it spans more than two decades, use a log scale\n",
    "        if float(max(counts)) / min(counts) > 100:\n",
    "            ax.set_yscale('log')\n",
    "\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(documents)\n",
    "word_list = cv.get_feature_names();\n",
    "count_list = cv_fit.toarray().sum(axis=0)\n",
    "word_hist = dict(zip(word_list, count_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "graph_builder = GraphMLCreator()\n",
    "#graph_builder.createGraphMLWithThreshold(co_result, word_hist, vocab, file=\"/Users/angieryu2202/Desktop/IF_App_Study/yes_response_coword_network_1.graphml\", threshold=1.0)\n",
    "graph_builder.createGraphMLWithThreshold(co_result, word_hist, vocab, file=\"/Users/angieryu2202/Desktop/IF_App_Study/no_response_coword_network_1.graphml\", threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_builder.plot_graph(file=\"/Users/angieryu2202/Desktop/IF_App_Study/if_app_review_coword_network.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
